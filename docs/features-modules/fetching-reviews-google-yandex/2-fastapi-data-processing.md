Раздел 2: Реализация Бэкенда: Центр Обработки Данных на FastAPI
В этом разделе подробно описывается создание серверной части приложения, включая проектирование схемы базы данных, разработку API-эндпоинта и логику скрипта для агрегации данных.

2.1. Проектирование Схемы Базы Данных (SQLite)
Основой для хранения данных служит единственная таблица reviews. Ее структура спроектирована так, чтобы быть независимой от источника данных, позволяя хранить отзывы из Google, Яндекса и потенциально других платформ в едином формате.

Ключевым аспектом схемы является использование полей, обеспечивающих целостность и отслеживаемость данных. Поле source четко указывает на происхождение отзыва, а original_review_id хранит уникальный идентификатор из исходной системы. Это позволяет реализовать надежный механизм дедупликации. Хранение даты публикации в виде publish_timestamp (UNIX-время) стандартизирует данные и упрощает сортировку, так как разные API могут возвращать дату в различных форматах (например, строка ISO 8601 у Google).

Для предотвращения дубликатов на уровне самой базы данных вводится ограничение UNIQUE на комбинацию полей (source, original_review_id). Это является дополнительным, более надежным уровнем защиты по сравнению с проверкой только на уровне Python-кода.

Таблица 1: Схема таблицы reviews

Имя столбца | Тип данных | Ограничения | Описание
id | INTEGER | PRIMARY KEY, AUTOINCREMENT | Уникальный внутренний идентификатор записи.
source | TEXT | NOT NULL | Источник отзыва (например, 'Google', 'Yandex').
author_name | TEXT | NOT NULL | Имя автора отзыва.
author_avatar_url | TEXT | NULLABLE | Прямая ссылка на изображение аватара автора.
rating | INTEGER | NOT NULL | Оценка в звездах (например, от 1 до 5).
text_content | TEXT | NULLABLE | Текст отзыва.
publish_timestamp | INTEGER NOT NULL | Дата и время публикации в формате UNIX timestamp.
original_review_id | TEXT | NOT NULL | Уникальный идентификатор отзыва из исходной системы.
created_at | DATETIME | DEFAULT CURRENT_TIMESTAMP | Дата и время добавления записи в локальную БД.
UNIQUE (source, original_review_id) Составной уникальный ключ для предотвращения дублей.

Export to Sheets
2.2. API-Эндпоинт: GET /api/reviews
Для предоставления данных фронтенду создается один эндпоинт, работающий только на чтение.

Спецификация эндпоинта:

Метод: GET

Путь: /api/reviews

Аутентификация: Не требуется (публичные данные).

Ответ (успех, 200 OK): Массив объектов JSON, где каждый объект представляет один отзыв.

В FastAPI этот эндпоинт реализуется с помощью функции-обработчика, которая выполняет запрос к базе данных SQLite. Для обеспечения типобезопасности и автоматической генерации документации определяется Pydantic-модель, структура которой полностью соответствует возвращаемым данным. Сервер выполняет предварительную сортировку данных по убыванию длины текста и затем по убыванию даты публикации (ORDER BY LENGTH(text_content) DESC, publish_timestamp DESC). Это позволяет снизить нагрузку на клиентское приложение, переложив основную часть логики сортировки на бэкенд, который имеет более быстрый доступ к данным.

2.3. Сервис Агрегации и Cron-Задача
Основная логика сбора данных выносится в отдельный Python-скрипт (cron_fetch_reviews.py), который будет запускаться по расписанию.

Алгоритм работы скрипта:

Инициализация: Скрипт импортирует необходимые библиотеки и загружает переменные окружения (API-ключи, идентификаторы) из файла .env.

Подключение к БД: Устанавливается соединение с файлом базы данных SQLite.

Получение существующих ID: Выполняется запрос SELECT source, original_review_id FROM reviews. Результат сохраняется в set для максимально быстрой проверки на дубликаты в памяти (O(1)).

Сбор данных из Google:

Инициализируется клиент Google API с использованием учетных данных сервисного аккаунта.

Вызывается метод accounts.locations.reviews.list для каждого location_id, указанного в конфигурации.

Полученный список отзывов передается в цикл обработки.

Сбор данных из Яндекса:

Инициализируется парсер из @/docs/features-modules/fetching-reviews-google-yandex/yandex-parser-py.md

Цикл нормализации и вставки (для каждого источника):

Для каждого полученного отзыва извлекается его оригинальный ID.

Проверяется, присутствует ли пара (source, original_review_id) в set, полученном на шаге 3. Если да, отзыв пропускается (continue).

Если отзыв новый, его данные нормализуются: поля из ответа API (например, reviewer.displayName, comment у Google) сопоставляются со столбцами таблицы reviews (author_name, text_content).

Извлекается URL аватара автора (reviewer.profilePhotoUrl у Google).

Дата публикации преобразуется в UNIX timestamp.

Формируется и выполняется SQL-запрос INSERT для добавления нового отзыва в базу данных.

Завершение: После обработки всех отзывов из всех источников транзакция подтверждается (commit), и соединение с базой данных закрывается.

Настройка Cron-задачи:
Для автоматического запуска этого скрипта в системный crontab на сервере добавляется следующая строка:

Bash

0 2 \* \* \* /usr/bin/python3 /path/to/your/project/backend/cron_fetch_reviews.py >> /path/to/your/project/logs/cron.log 2>&1
Эта команда будет запускать скрипт каждый день в 2 часа ночи, а весь вывод (как стандартный, так и ошибки) будет записываться в лог-файл для последующего анализа.
